{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-09-25T14:26:57.175162Z","iopub.status.busy":"2022-09-25T14:26:57.174389Z","iopub.status.idle":"2022-09-25T14:26:58.490115Z","shell.execute_reply":"2022-09-25T14:26:58.487865Z","shell.execute_reply.started":"2022-09-25T14:26:57.175052Z"},"trusted":true},"outputs":[],"source":["import numpy as np \n","import pandas as pd \n","from sklearn.model_selection import train_test_split\n","import sys  \n","import random\n","import math "]},{"cell_type":"markdown","metadata":{},"source":["# 預處理\n","讀取dataset以及進行簡單預處理。"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-09-25T14:27:06.178281Z","iopub.status.busy":"2022-09-25T14:27:06.177832Z","iopub.status.idle":"2022-09-25T14:27:06.195159Z","shell.execute_reply":"2022-09-25T14:27:06.193590Z","shell.execute_reply.started":"2022-09-25T14:27:06.178236Z"},"trusted":true},"outputs":[],"source":["data = pd.read_csv(\"./input/Iris.csv\")                      \n","                                                                  \n","data = data.to_numpy()                                            \n","data = data[:,1:] #　remove column 'Id'                            \n","train_dataset, val_dataset = train_test_split(data, test_size=0.2)"]},{"cell_type":"markdown","metadata":{},"source":["# 建立decision tree的類別\n","我一共建立了Node與DecisionTree兩個類別。"]},{"cell_type":"markdown","metadata":{},"source":["## class Node\n","\n","我將節點分為internal node（root node也被歸類於此）以及leaf node兩種。\n","\n","### Internal node\n","Internal node用於將輸入資料分類（進行決策），假設輸入資料為`X`，我們可以依據輸入資料的特徵`X[feature_idx]`與是否小於等於閥值`threshold`，來決定輸入資料`X`要往左子節`left`或右子節點`right`前進。\n","\n","### Leaf node\n","當有資料經過決策（internal nodes）後來到leaf node的話，則代表該筆資料被分類到第`class_type`個種類。"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-09-24T02:16:03.328101Z","iopub.status.busy":"2022-09-24T02:16:03.327440Z","iopub.status.idle":"2022-09-24T02:16:03.335218Z","shell.execute_reply":"2022-09-24T02:16:03.333806Z","shell.execute_reply.started":"2022-09-24T02:16:03.328052Z"},"trusted":true},"outputs":[],"source":["# class of node\n","class Node:\n","    def __init__(self, left=None, right=None, feature_idx=None, threshold=None, class_type=None):\n","        # Only be used at internal node\n","        self.left = left\n","        self.right = right\n","        self.feature_idx = feature_idx\n","        self.threshold = threshold\n","        \n","        # Only be used at leaf node\n","        self.class_type = class_type\n","        \n","        "]},{"cell_type":"markdown","metadata":{},"source":["## class DecisionTree\n","\n","建立`DecisionTree`之初，我們會先定義幾個進行擬合時會用到的參數，這些參數會影響到樹是否會over/under fitting，以及影響擬合時間。參數如下：\n","\n","* `max_depth`\n","    定義樹的最大深度\n","* `iter_feature_cnt`\n","    若選擇過大的`iter_feature_cnt`，代表每次增加節點時會考慮較多的特徵與可能的閥值，這可能導致擬合時間過長以及over fitting的發生。但若`iter_feature_cnt`過小則可能導致under fitting。\n","* `min_sample_cnt`\n","    進行擬合時，若節點的data數量小於`min_sample_cnt`則不繼續進行分割。"]},{"cell_type":"markdown","metadata":{},"source":["我也將Gini Impurity與entropy列於下方：\n","### Gini Impurity\n","$$\\displaystyle GiniImpurity=1-\\sum _{i=1}^{J}p_{i}^{2}$$\n","\n","### Entropy\n","$$\\displaystyle Enttopy=-\\sum p(x)\\log \\left({p(x)}\\right)$$"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-09-24T02:16:03.338824Z","iopub.status.busy":"2022-09-24T02:16:03.338358Z","iopub.status.idle":"2022-09-24T02:16:03.370355Z","shell.execute_reply":"2022-09-24T02:16:03.369021Z","shell.execute_reply.started":"2022-09-24T02:16:03.338780Z"},"trusted":true},"outputs":[],"source":["# class of DecisionTree\n","class DecisionTree:\n","    def __init__(self, max_depth, iter_feature_cnt, min_sample_cnt, information_type):\n","        self.root = None\n","        self.dice = None\n","        self.max_depth = max_depth\n","        # How many features will be choosed in each iteration of creating a new node\n","        # Larger iter_feature_cnt can get better precision. But it will need more time spending on fitting.\n","        self.iter_feature_cnt = iter_feature_cnt\n","        # If the sample count is less than min_sample_cnt, stop making decision.\n","        self.min_sample_cnt = min_sample_cnt\n","        self.information_type = information_type\n","        \n","    def fit(self, dataset):\n","        self.tree_init(dataset)\n","        self.root = self.create_node(dataset, 0)\n","        \n","    def tree_init(self, dataset):\n","        # The dice is used for randomly pick feature\n","        self.dice = range(dataset.shape[1] - 1)\n","    \n","    def create_node(self, dataset, curr_depth):\n","        # Use recursion to create the whole decision tree\n","        if curr_depth <= self.max_depth and len(dataset) >= self.min_sample_cnt and len(np.unique(dataset[:,-1])) > 1:\n","            ig, feature_idx, threshold, left_dataset, right_dataset = self.best_split(dataset)\n","            # Avoid dicision tree doesn't be splited\n","            # If information gain equals to 0, that means the dataset doesn't be splited\n","            if ig > 0: \n","                # Create internal node\n","                return Node(left=self.create_node(left_dataset, curr_depth+1),\n","                            right=self.create_node(right_dataset, curr_depth+1),\n","                            feature_idx=feature_idx, \n","                            threshold=threshold)\n","\n","        # Current node is leaf node if current node doesn't need to be splited\n","        # Pick out the most frequent type in current dataset as the predicted type\n","        values, counts = np.unique(dataset[:,-1], return_counts=True)\n","        most_frequent_type = values[np.argmax(counts)]\n","        return Node(class_type=most_frequent_type)\n","\n","    def roll_dice(self):\n","        # Used for randomly choose features\n","        self.dice = random.sample(self.dice, len(self.dice))\n","        \n","    def best_split(self, dataset):\n","        self.roll_dice()\n","        max_ig = -1 * sys.float_info.max\n","        max_feature_idx = None\n","        max_threshold = None\n","        \n","        # Pick out the feature and threshold with largest infromation gain as desicion \n","        for i in range(self.iter_feature_cnt):\n","            curr_feature_idx = self.dice[i]\n","            for curr_threshold in np.unique(dataset[:,curr_feature_idx]):\n","                left_dataset, right_dataset = self.split(dataset, curr_feature_idx, curr_threshold)\n","                if len(left_dataset) > 0 and len(right_dataset) > 0:\n","                    curr_ig = self.compute_ig(dataset[:,-1], left_dataset[:,-1], right_dataset[:,-1])\n","                    if curr_ig > max_ig:\n","                        max_left_dataset, max_right_dataset = left_dataset, right_dataset\n","                        max_feature_idx = curr_feature_idx\n","                        max_threshold = curr_threshold\n","                        max_ig = curr_ig\n","        return max_ig, max_feature_idx, max_threshold, max_left_dataset, max_right_dataset\n","                    \n","                \n","    def split(self, dataset, feature_idx, threshold):\n","        # If the value of the feature is less and equal than threshold, splited to left_dataset. Otherwise, splited to right_dataset.\n","        left_dataset = np.array([row for row in dataset if row[feature_idx]<=threshold])\n","        right_dataset = np.array([row for row in dataset if row[feature_idx]>threshold])\n","        \n","        return left_dataset, right_dataset\n","    \n","    def compute_ig(self, y, left_y, right_y):\n","        weight_l = len(left_y) / len(y)\n","        weight_r = len(right_y) / len(y)\n","        if self.information_type == 'gini':\n","            return self.gini(y) - weight_l * self.gini(left_y) - weight_r * self.gini(right_y)\n","        elif self.information_type == 'entropy':\n","            return self.entropy(y) - weight_l * self.entropy(left_y) - weight_r * self.entropy(right_y)\n","        else:\n","            raise ValueError(\"Parameter 'information_type' for DecisionTree.compute_ig is invalid\")\n","        \n","    def entropy(self, y):\n","        data_cnt = len(y)\n","        info = 0\n","        for cls in np.unique(y):\n","            p_x = (len(y[y == cls]) / data_cnt)\n","            info -= p_x * math.log(p_x)\n","        return info\n","            \n","    def gini(self, y):\n","        info = 1\n","        data_cnt = len(y)\n","        for cls in np.unique(y):\n","            info -= (len(y[y == cls]) / data_cnt) ** 2\n","        return info\n","    \n","    def predict_all(self, X):\n","        y = [self.predict_single(x) for x in X]\n","        return y\n","    \n","    def predict_single(self, x):\n","        curr_node = self.root\n","\n","        while curr_node.class_type == None:\n","            if x[curr_node.feature_idx] <= curr_node.threshold:\n","                curr_node = curr_node.left\n","            else:\n","                curr_node = curr_node.right\n","        return curr_node.class_type\n","    \n","    def print_precision(self, y, pred_y):\n","        print(len([y[i] for i in range(len(y)) if y[i]==pred_y[i]]) / len(y))\n","    \n","    def print_tree(self):\n","        \n","        # Create binary tree\n","        binary_tree_list = [[self.root]]\n","        depth = 0\n","        while len(binary_tree_list) > depth:\n","            l = []\n","            for node in binary_tree_list[depth]:\n","                if node != None:\n","                    l.append(node.left)\n","                    l.append(node.right)\n","                else:\n","                    l.append(None)\n","                    l.append(None)\n","            # If there ara at least one node in current depth\n","            if len(l) > len([ele for ele in l if ele == None]):\n","                binary_tree_list.append(l)\n","            depth += 1\n","        \n","        # Print tree\n","        depth = 0\n","        for node_list in binary_tree_list:\n","            print('Depth: ' + str(depth))\n","            node_idx = 0\n","            for node in node_list:\n","                if node != None:\n","                    print('Node index: ' + str(node_idx) + '\\t Feature index: ' + str(node.feature_idx), '\\t Threshold: ' + str(node.threshold))\n","                node_idx += 1\n","            depth += 1\n","            print()\n","            "]},{"cell_type":"markdown","metadata":{},"source":["# 擬合與訓練\n","\n","最後則是擬合與訓練的部分，各位可以自己調整以下幾個參數，來看對訓練成效的影響。我自己試了一下，要選取比較極端的數值（如`iter_feature_cnt=1`），才有可能讓precision比較低。\n","\n","這裡我簡單的寫了一個print函式，讓各位可以簡單地看一下訓練出來的樹會是什麼樣子。\n","我將我們訓練出來的樹用complete binary tree的方式來print出來。假設深度=2且node index = 3的節點，其父節點為深度=2<b><font color=#FF0000>-1</font></b>=1且node index = 3<b><font color=#FF0000>//2</font></b>=1。也就是說，我們可以<b><font color=#FF0000>藉由將子節點的深度減1且node index除2來得到父節點的深度與node index</b>。"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-09-24T02:19:35.681937Z","iopub.status.busy":"2022-09-24T02:19:35.681434Z","iopub.status.idle":"2022-09-24T02:19:35.754809Z","shell.execute_reply":"2022-09-24T02:19:35.753489Z","shell.execute_reply.started":"2022-09-24T02:19:35.681884Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0.8666666666666667\n","Depth: 0\n","Node index: 0\t Feature index: 2 \t Threshold: 1.9\n","\n","Depth: 1\n","Node index: 0\t Feature index: None \t Threshold: None\n","Node index: 1\t Feature index: 3 \t Threshold: 1.7\n","\n","Depth: 2\n","Node index: 2\t Feature index: None \t Threshold: None\n","Node index: 3\t Feature index: None \t Threshold: None\n","\n"]}],"source":["max_depth = 5\n","iter_feature_cnt = 4 # This number can't larger than 4. It will lead to error.\n","min_sample_cnt = 50\n","information_type = 'gini' # gini or entropy\n","\n","\n","tree = DecisionTree(max_depth, iter_feature_cnt, min_sample_cnt, information_type)\n","tree.fit(train_dataset)\n","pred_y = tree.predict_all(val_dataset)\n","tree.print_precision(val_dataset[:,-1], pred_y)\n","tree.print_tree()"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.4 ('pytorch')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"9d07ada2db3a750ea7525ef0abae1dd8d2e024d709104a053d6ba9a14985ab36"}}},"nbformat":4,"nbformat_minor":4}
